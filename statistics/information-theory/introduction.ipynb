{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 style=\"text-align:center; bold\">\n",
    "Information Theory: Introduction\n",
    "</h1>\n",
    "\n",
    "<p style=\"text-align:center;\">&nbsp Information Theory is a field comprised of elements from mathematics, statistics, signal processing, and computer science. It was\n",
    "developed by Claude Shannon to handle signal communications between a theoretical sender, reciever, channel and medium.</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "**Basic Signal Communication Model**\n",
    "---\n",
    "<br>\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "\n",
    "![Signal Communication](./resources/SigModel.png)\n",
    "\n",
    "</div>\n",
    "\n",
    "The modelling example below can show the different components to the founding principle of Information Theory; the science of operations on data. Claude Shannon, the father of the field, started everything with a paper entitled, \"A Mathematical Theory of Communication\"\n",
    "\n",
    "**1.1 Examples of Operations on data**\n",
    "  - Compression\n",
    "  - Storage\n",
    "  - Communication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Lossless Compression**\n",
    "---\n",
    "\n",
    "Consider we have some signal comprised of signals $U_0$ through $U_n$ who are independently and indentically distributed (i.i.d): \n",
    "\n",
    "  - $U_1, U_2, ..., U_n$\n",
    "  - $U_i$ $\\epsilon$ $[a, b, c]$ \n",
    "\n",
    "This signal is given by the P.M.F:\n",
    "\n",
    "  - $P(U=a) = 0.7$\n",
    "  - $P(U=b) = P(U=c) = 0.15$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Problem:**\n",
    "\n",
    "We want to encode the source sequence ($U_i$ $\\epsilon$ $[a,b,c]$) in a binary representation. How would you do it?\n",
    "\n",
    "**Answer 1-a: Naive Answer**\n",
    "\n",
    "We have three possible combinations: a, b, or c. Therefore we need 2 binary digits to represent the three possible combinations:\n",
    "\n",
    "  - a $\\rightarrow$ 00\n",
    "  - b $\\rightarrow$ 01\n",
    "  - c $\\rightarrow$ 10\n",
    "\n",
    "This leaves us with an expected word length of 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1-b: Truncate a / The prefix condition**\n",
    "\n",
    "One thing we could do is truncate our representation of a. We have 00, with no other representation for 0, so we set a $\\rightarrow$ 0. One option for removing ambiguity in codes is to satisfy the prefix condition: No code can be the prefix of another code. To do this we could take b $\\rightarrow$ 10, and c $\\rightarrow$ 11. In the options above (with a $\\rightarrow$ 0), we could receive the code 010 \\\n",
    "and we would not know if it meant \"ac\" or \"ba\", our code does not have this problem.\n",
    "\n",
    "We can calculate the expected number of bits per symbol recieved (denoted $\\bar{L}$) can be calculated using:\n",
    "\n",
    "  - $\\sum len(W_i) \\frac{count(W_i)}{len(W_0 : W_n)}$\n",
    "\n",
    "$\\bar{L}_b = 1 * P(U_i = a) + 2 * (P(U_i = b) + P(U_i = c)) = 1 * 0.7 + 2 * (0.15 + 0.15) = 1.3$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer 1-c: Lossless Compression**\n",
    "\n",
    "These approaches so far have been good but we can do better. If we apply the same principles as before, but with the added improvement of: \"Increase the length of a symbol encoding according to the probability of that symbol occuring\". To do this see the table below for the encodings, probabilities, and symbols. Remember we must satisfy the prefix condition. To do \\\n",
    "this we can take all pairs of two symbols inside our language:\n",
    "\n",
    "**Encoding Probabilities**\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|} \n",
    "\\hline \\text{Source Symbols} & \\text{counts} & \\text{Prob.}\\\\ \n",
    "\\hline \\text{aa} & P(U_i = aa) = P(U_i = a)^2 & 0.49\\\\ \n",
    "\\hline \\text{ab} & P(U_i = ab) = P(U_i = a)*P(U_i = b) & 0.105\\\\\n",
    "\\hline \\text{ac} & P(U_i = ac) = P(U_i = a)*P(U_i = c) & 0.105\\\\\n",
    "\\hline \\text{ba} & P(U_i = ba) = P(U_i = b)*P(U_i = a) & 0.105\\\\\n",
    "\\hline \\text{ca} & P(U_i = ca) = P(U_i = c)*P(U_i = a) & 0.105\\\\\n",
    "\\hline \\text{bb} & P(U_i = bb) = P(U_i = b)^2 & 0.0225\\\\\n",
    "\\hline \\text{bc} & P(U_i = bc) = P(U_i = b)*P(U_i = c) & 0.0225\\\\\n",
    "\\hline \\text{cc} & P(U_i = cc) = P(U_i = c)^2 & 0.0225\\\\\n",
    "\\hline \\text{cb} & P(U_i = cb) = P(U_i = c)*P(U_i = b) & 0.0225\\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "**Encodings**\n",
    "\n",
    "$$\n",
    "\\begin{array}{|c|c|c|c|} \n",
    "\\hline \\text{Source Symbols} & \\text{Prob.} & \\text{Encoding} \\\\ \n",
    "\\hline \\text{aa} & 0.49 & 0 \\\\\n",
    "\\hline \\text{ab} & 0.105 & 100 \\\\\n",
    "\\hline \\text{aa} & 0.105 & 111 \\\\\n",
    "\\hline \\text{aa} & 0.105 & 101 \\\\\n",
    "\\hline \\text{aa} & 0.105 & 1100 \\\\\n",
    "\\hline \\text{aa} & 0.0225 & 110100 \\\\\n",
    "\\hline \\text{aa} & 0.0225 & 110101 \\\\\n",
    "\\hline \\text{aa} & 0.0225 & 110110 \\\\\n",
    "\\hline \\text{aa} & 0.0225 & 110111 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "These encodings satisfy both the prefix condition, and our extra condition that the most frequent encodings have the smallest encodings. These would be an example of Huffman Encodings, and our minimal expected number of bits can be calculated using:\n",
    "\n",
    "$\\bar{L}_c$ $=$ $0.5(0.49(1) + 4(0.105)(3) + 4(0.0225)(6))$ $=$ $1.1975$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entropy**\n",
    "---\n",
    "\n",
    "Entropy is one of the most useful measurements to come out of Information Theory. It measures the amount of information shared between two signal sources. Entropy is derived from how we answered question 1, the lowest expected number of bits to represent source A in source B. We can calculate Entropy (denoted $H(U)$) using:\n",
    "\n",
    "$H(U) = \\sum{p(u) log_2 \\frac{1}{p(u)}}$\n",
    "\n",
    "Since we derived Entropy from the minimal expected codeword length, Shannon proved we can bound the expected code word length using the following: (where $\\epsilon$ represents some expected error bound $\\gt$ 0)\n",
    "\n",
    "  - $\\bar{L} \\geq H(u)$\n",
    "  - $\\bar{L} \\leq H(u) + \\epsilon$\n",
    "\n",
    "**Final Bound on Expected Codeword Length**\n",
    "\n",
    "$H(u) \\leq \\bar{L} \\leq H(u) + \\epsilon$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML-Env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
